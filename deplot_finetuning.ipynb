{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /opt/homebrew/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from gdown) (3.8.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/homebrew/lib/python3.10/site-packages (from gdown) (2.28.1)\n",
      "Requirement already satisfied: six in /opt/homebrew/lib/python3.10/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from gdown) (4.65.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.10/site-packages (from gdown) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->gdown) (2022.9.24)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/homebrew/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, Pix2StructForConditionalGeneration\n",
    "from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from evaluate import load\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding `max_patches` argument\n",
    "\n",
    "The paper introduces a new paradigm for processing the input image. It takes the image and create `n_patches` aspect-ratio preserving patches, and concatenates the remaining sequence with padding tokens to finally get `max_patches` patches. It appears that this argument is quite crucial for training and evaluation, as the model becomes very sensitive to this parameter.\n",
    "\n",
    "For the sake of our example, we will fine-tune a model with `max_patches=1024`.\n",
    "\n",
    "Note that most of the `-base` models have been fine-tuned with `max_patches=2048`, and `4096` for `-large` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/matcha-base\")\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/matcha-base\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ChartQA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PATCHES = 2048\n",
    "\n",
    "class ChartQADataset(Dataset):\n",
    "    def __init__(self, processor, root_dir=\"ChartQA Dataset\", split='train', split2=\"both\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the ChartQA data.\n",
    "            split (string): Which split to load (\"train\" or \"val\" or \"test\").\n",
    "            split2 (string): Which split to load (\"both\" or \"augmented\" or \"human\") within the first split.\n",
    "        \"\"\"\n",
    "        self.processor = processor\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.image_dir = os.path.join(self.root_dir, self.split, 'png')\n",
    "        \n",
    "        self.qa_augmented = []\n",
    "        self.qa_human = []\n",
    "        # Load questions and answers\n",
    "        with open(os.path.join(self.root_dir, self.split, f'{split}_augmented.json'), 'r',  encoding='utf-8') as f:\n",
    "            self.qa_augmented = json.load(f)\n",
    "        with open(os.path.join(self.root_dir, self.split, f'{split}_human.json'), 'r', encoding='utf-8') as f:\n",
    "            self.qa_human = json.load(f)\n",
    "\n",
    "        if split2 == \"both\":\n",
    "            self.data = self.qa_augmented + self.qa_human\n",
    "        elif split2 == \"augmented\":\n",
    "            self.data = self.qa_augmented\n",
    "        elif split2 == \"human\":\n",
    "            self.data = self.qa_human\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.qa_augmented) + len(self.qa_human)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        qa = self.data[idx]\n",
    "        # Load image\n",
    "        qa[\"image\"] = Image.open(f\"{self.image_dir}/{qa['imgname']}\").convert('RGB')\n",
    "        return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(batch):\n",
    "  new_batch = {\"flattened_patches\":[], \"attention_mask\":[]}\n",
    "  images = [item[\"image\"] for item in batch]\n",
    "  header_texts = [item[\"query\"] for item in batch]\n",
    "  label_texts = [item['label'] for item in batch]\n",
    "  \n",
    "  inputs = processor(images=images, text=header_texts, padding=\"max_length\", return_tensors=\"pt\", add_special_tokens=True, max_length=128)\n",
    "  labels = processor.tokenizer(label_texts, return_tensors=\"pt\", padding=\"max_length\", max_length=128)\n",
    "  new_batch[\"labels\"] = labels.input_ids\n",
    "  new_batch[\"flattened_patches\"] = inputs[\"flattened_patches\"]\n",
    "  new_batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "  return new_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the processor, let's load the dataset and the dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChartQADataset(processor, split='train')\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ChartQADataset(processor, split='val', split2=\"human\")\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_steps = 10000\n",
    "checkpoint_steps = (256/batch_size)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(model.parameters(), scale_parameter=False, relative_step=False, lr=0.01, weight_decay=1e-05)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1000, num_training_steps=(256/batch_size)*10000)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 3.9150619506835938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[45], line 17\u001b[0m\n",
      "\u001b[1;32m     14\u001b[0m flattened_patches \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mflattened_patches\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     15\u001b[0m attention_mask \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;32m---> 17\u001b[0m outputs \u001b[39m=\u001b[39m model(flattened_patches\u001b[39m=\u001b[39;49mflattened_patches,\n",
      "\u001b[1;32m     18\u001b[0m                 attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m     19\u001b[0m                 labels\u001b[39m=\u001b[39;49mlabels)\n",
      "\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:1713\u001b[0m, in \u001b[0;36mPix2StructForConditionalGeneration.forward\u001b[0;34m(self, flattened_patches, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, labels, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m   1711\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n",
      "\u001b[1;32m   1712\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m-> 1713\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n",
      "\u001b[1;32m   1714\u001b[0m         flattened_patches\u001b[39m=\u001b[39;49mflattened_patches,\n",
      "\u001b[1;32m   1715\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m   1716\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n",
      "\u001b[1;32m   1717\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m   1718\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n",
      "\u001b[1;32m   1719\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n",
      "\u001b[1;32m   1720\u001b[0m     )\n",
      "\u001b[1;32m   1722\u001b[0m hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m   1724\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m   1725\u001b[0m     \u001b[39m# get decoder inputs from shifting lm labels to the right\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:637\u001b[0m, in \u001b[0;36mPix2StructVisionModel.forward\u001b[0;34m(self, flattened_patches, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    633\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n",
      "\u001b[1;32m    635\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(flattened_patches)\n",
      "\u001b[0;32m--> 637\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n",
      "\u001b[1;32m    638\u001b[0m     embedding_output,\n",
      "\u001b[1;32m    639\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n",
      "\u001b[1;32m    640\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n",
      "\u001b[1;32m    641\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n",
      "\u001b[1;32m    642\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n",
      "\u001b[1;32m    643\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n",
      "\u001b[1;32m    644\u001b[0m )\n",
      "\u001b[1;32m    645\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    646\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:360\u001b[0m, in \u001b[0;36mPix2StructVisionEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m    353\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n",
      "\u001b[1;32m    354\u001b[0m         create_custom_forward(layer_module),\n",
      "\u001b[1;32m    355\u001b[0m         hidden_states,\n",
      "\u001b[1;32m    356\u001b[0m         attention_mask,\n",
      "\u001b[1;32m    357\u001b[0m         layer_head_mask,\n",
      "\u001b[1;32m    358\u001b[0m     )\n",
      "\u001b[1;32m    359\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 360\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n",
      "\u001b[1;32m    362\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:313\u001b[0m, in \u001b[0;36mPix2StructVisionLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n",
      "\u001b[1;32m    311\u001b[0m \u001b[39m# in Pix2StructVision, layernorm is also applied after self-attention\u001b[39;00m\n",
      "\u001b[1;32m    312\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_mlp_layer_norm(hidden_states)\n",
      "\u001b[0;32m--> 313\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(layer_output) \u001b[39m+\u001b[39m hidden_states  \u001b[39m# second residual connection\u001b[39;00m\n",
      "\u001b[1;32m    315\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n",
      "\u001b[1;32m    317\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/transformers/models/pix2struct/modeling_pix2struct.py:258\u001b[0m, in \u001b[0;36mPix2StructVisionMlp.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n",
      "\u001b[1;32m    257\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n",
      "\u001b[0;32m--> 258\u001b[0m     hidden_gelu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi_0(hidden_states))\n",
      "\u001b[1;32m    259\u001b[0m     hidden_linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwi_1(hidden_states)\n",
      "\u001b[1;32m    260\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_gelu \u001b[39m*\u001b[39m hidden_linear\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n",
      "\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for idx, batch in cycle(enumerate(train_dataloader)):\n",
    "  labels = batch.pop(\"labels\").to(device)\n",
    "  flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "\n",
    "  outputs = model(flattened_patches=flattened_patches,\n",
    "                  attention_mask=attention_mask,\n",
    "                  labels=labels)\n",
    "  \n",
    "  loss = outputs.loss\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  print(f\"Step {idx+1}/{training_steps} - Loss: {loss.item()}\")\n",
    "\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  if idx+1 % checkpoint_steps == 0:\n",
    "      model.eval()\n",
    "\n",
    "      val_loss = []\n",
    "      val_batch_size = []\n",
    "      for batch in val_dataloader:\n",
    "        val_labels = batch.pop(\"labels\").to(device)\n",
    "        val_flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "        val_attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "        outputs = model(flattened_patches=flattened_patches,\n",
    "                  attention_mask=attention_mask,\n",
    "                  labels=labels)\n",
    "  \n",
    "        loss = outputs.loss\n",
    "        curr_val_loss = loss.item()\n",
    "        val_loss.append(curr_val_loss)\n",
    "        val_batch_size.append(val_labels.size(0))\n",
    "      val_loss_average = [v_loss*batch_size for v_loss, batch_size in zip(val_loss, val_batch_size)] / sum(val_batch_size)\n",
    "      print(f\"Validation Loss: {val_loss_average}\")\n",
    "      checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "      }\n",
    "      torch.save(checkpoint, f'{checkpoint_dir}/checkpoint_training_step_{idx+1}_val_loss_{val_loss_average}.pth')\n",
    "\n",
    "      model.train()\n",
    "  \n",
    "  if idx+1 == training_steps:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results on our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ChartQADataset(processor, split='test')\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_metric = load(\"exact_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "aug_accuracy = []\n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "  labels = batch.pop(\"labels\").to(device)\n",
    "  flattened_patches = batch.pop(\"flattened_patches\").to(device)\n",
    "  attention_mask = batch.pop(\"attention_mask\").to(device)\n",
    "\n",
    "  generated_ids = model.generate(flattened_patches=flattened_patches, attention_mask=attention_mask, max_length=128)\n",
    "  print(processor.tokenizer.batch_decode(generated_ids,skip_special_tokens=True))\n",
    "\n",
    "  metric = exact_match_metric.compute(predictions=generated_ids, references=labels)\n",
    "  print(metric)\n",
    "  break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
